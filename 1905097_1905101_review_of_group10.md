# [Shadowcast: Stealthy Data Poisoning Attacks Against Vision-Language Models](https://arxiv.org/abs/2402.06659)

Reviewer: Abrar Mahmud(1905097), MD Sadik Hossain Shanto(1905101)

The blog on **Shadowcast** provides a comprehensive overview of a novel data poisoning attack targeting Vision-Language Models (VLMs). Shadowcast introduces a stealthy method that manipulates VLMs by using visually indistinguishable poisoned images, which can lead to significant vulnerabilities in how these models interpret and generate text based on images.

Data poisoning attacks involve the deliberate injection of misleading data into a model's training set, aiming to corrupt its learning process. In the case of **Shadowcast**, the attack is designed to mislead VLMs into associating images from one concept (the original concept, denoted as *C₀*) with another concept (the destination concept, denoted as *Cₐ*). This manipulation can result in the generation of inaccurate or misleading narratives.


The blog details two primary types of attacks:

### 1. Label Attack
This attack aims to manipulate the model into misclassifying identities. For example, when the model encounters an image of Donald Trump, it might incorrectly generate responses that associate him with Joe Biden instead. This type of attack can have significant implications in contexts like social media and news, where accurate identification is crucial.

### 2. Persuasion Attack
In this case, the goal is to generate convincing but false narratives. For instance, an image representing junk food could be manipulated to be described as *"healthy food rich in nutrients."* This attack leverages the text generation capabilities of VLMs to create misleading descriptions that could influence public perception and behavior.

## Key Contributions of the Paper

- **Introduction of Shadowcast**: Shadowcast is the first data poisoning attack specifically designed for VLMs, demonstrating how visually indistinguishable poisoned images can manipulate model responses without detection.
- **Stealthy Approach**: The authors detail a method for crafting poisoned image/text pairs that are visually congruent, ensuring that the injected data appears benign and aligns with the model's training data.
- **Resilience**: The experiments conducted demonstrate that Shadowcast is effective across various VLM architectures and prompts, showcasing its robustness against different model configurations.
- **Future Research Directions**: This highlights the urgent need for research into defense mechanisms against such poisoning attacks, emphasizing the challenges posed by the unique characteristics of VLMs.

Overall, this blog is a must-read for anyone interested in AI, privacy and security of ML, or the ethical implications of technology. It’s informative, well-structured, and thought-provoking, making it a valuable contribution to the ongoing conversation about the safety of VLMs. 
